# Проектирование высоконагружнных приложений

## Тема и целевая аудитория

Тема - "Проектирование сервиса по типу Google Drive"

Функционал (MVP) :
- загрузка файла
- удаление файла
- скачивание файла
- поиск по диску
- создание общей папки, файла
- история изменения папок, файлов

Целевая аудитория:
- 2.6B Посещений в месяц по всему миру
- 85.5M Посещений в день
- Среднее время визита 6 min
- Среднее количество страниц за визит 5
- Bounce Rate(Cредний процент посетителей, которые просматривают только одну страницу) - 29%
- Основные пользователи офисные сотрудники и студенты, возраст 20-40 лет
- Распредение по странам
  | Страна         | Процент от всех юзеров   |
  | -------------- | ------------------------ |
  | США            | ~25 %   |
  | Бразилия       | ~6 %    |
  | Индия          | ~4.5 %  |
  | Индонезия      | ~3 %    |
  | Мексика        | ~3 %    |
  | Япония         | ~3%     |
Источники: [hypestat](https://hypestat.com/info/drive.google.com), [simularweb](https://www.similarweb.com/website/drive.google.com/#demographics)

## Расчет нагрузки

### Объем хранилища и типы файлов
В стандартном тарифном плане объем предоставлемого хранилища 15 гб для компаний и платных пользоватей 2тб. Платных пользоватей у Google drive 8mln. Вообщем пользоватей примерно 2.6bln. На основании собственного опыта и опроса знакомых диск в среднем заполнеy на 2/3. Примерно по пропорции можно закладывать 18GB в среднем на пользователя.

**Общий размер всего хранилища**: ```18GB * 2.6bln ~ 41_600PiB.```

По опросам пользователей, 80% используют диск только для видео и фото. Для простоты будем счиать что остальные файлы это документы. Поскольку основной алгоритм использования это загрузка на диск фото и видео с телефона для долгострочного хранения. Соотношение фото к видео у меня и знакомых на телефоне примерное 8:1, будем брать такую статистику в расчетах.

  | Тип файла         |Процент от всех файлов в штуках    |
  | -------------- | ------------ |
  | Фото           | 71 %        |
  | Видео          | 9 %         |
  | Документы      | 20 %       |


Оценим средний размер PNG картинки ```( 1920px * 1080 px * 4b + 33b (header) ) * 0.66 (среднее сжатие DEFLATE) ~= 5.12 MiB```. Также есть информация что средний размер фотографии на iPhone в 2023 году был 2-8mb, что примерно бьется с расчетами.
Продолжая ориентироватся что большая часть /фото видео снимается на iPhone возьмем средний размер 1080p/30fps видео снятого на последний - это ```60MiB/min```, средняя продолжительность видео на самом попярном видеохостинге ```11.7min```. Исходя из этого средний обьем видео:```60 * 11.7 = 700MiB```
Размер документа docx/pdf   ```10 kB + 3kB/page```, будем брать среднем 30KiB.
Итого:

  | Тип файла      |Средний размер  |
  | -------------- | -------------- |
  | Фото           | 5.12MiB        |
  | Видео          | 700MiB         |
  | Документы      | 30KiB          |



### Среднее количество действий пользователя день:

Приведем статистику которая есть от похожего сервиса - [Dropbox](https://marketsplash.com/dropbox-statistics/#link4).
 - 1.2bln загружаемых файлом в день при DAU 4mln, в пересчете 300 файлов в день на пользователя
 - 100k создаваемых ссылок на папки, файлы при DAU 4mln в пересчете 0.6 ссылок в день на пользователя
   

Просмотр файла также будем считать за скачивание, так как пользователь пользователь просматривая картинку/видео/документ его скачивает.
Пусть около 5% посещений происходит с запросом авторизации. Среднее количество посещаемых страниц равно 5, а поскольку на диске только папки являются страницами будем брать это как просмотр содержимого папки. Будем считать что просматривают пользователи в 2 раза больше чем загружают, к-во удалений тоже возьмем сравнимое с загрузкой.


| Действие    | Среднее количество дейстивий в день на пользователя |
| --------    | ------- |
| Аутентификация    | 0.05 |
| Аутентификация по куке | 1 |
| Загрузка на диск | 300 |
|Просмотр/скачивание с диска| 600|
| Удаление | 100 |
|Создание ссылки| 0.6 |
| Просмотр директории | 5 |
| Поиск по диску | 5 |


### RPS
DAU = 85.5 M
RPS = количесво действий на пользователя * DAU / 86 400

Файлы будут загрузаться чанками по 10 MiB с клиента, чанкование возможно в основном только для видео в нашем случае это ```700/10 = 70``` запросов на загрузку видео. Количество загрузок видео ```300*9% =27```.

```RPS = DAU * (0.05 + 300*91% + 27 * 70 + 600 + 100 + 0.6 + 5 + 5) / 86400 = 2873.65 * DAU /86400 ~ 2_850_000```

Общее
- **RPS** ~ 2_850_000.
- **Пиковый RPS (x1.5)** ~ 4_250_000.

**RPS по типам запросов**
| Запрос    | RPS |
|---|---|
| Аутентификация  | 50 |
| Аутентификация по куке | 1_000|
| Загрузка на диск | 300_000 |
| Скачивание с диска | 600_000 |
| Создание сслыки/изменение доступа |600|
| Просмотр директории | 5_000 |
| Поиск по диску | 5_000 |

### Расчет сетевого траффика
DAU = 85.5 M

Скачивание будет производится с s3 так что посчитаем его отдельно. Все запросы не связанные с файлами будем счиать размером в 5KiB

**Общий трафик**
```Дневной трафик = DAU * sum(count*size) * 8```

```DAU * ((0.05+600+100+0.6 + 5 + 5) * 5KiB + 300*20%*30KiB + 27*700MiB + 300*71%*5.12MiB) * 8 = DAU * (3.47MiB + 1.76MiB + 18900MiB + 1090.56MiB) * 8 ~ 12_738 Pbit/day```

``` Трафик с секунду = 12_738 Pbit/day / 86400 = 151 Tbit/sec```


**Скачивание**

``` Скачивание в день = DAU * 600 * (0.2 * 30KiB + 0.09 * 700MiB + 0.71 * 5.12MiB) * 8 = 25_491 Pbit/day ```

``` Скачивание в секунду =  25_491 Pbit/day / 86400 = 302Tbit/sec```



### Технические метрики:
| Метрика    | Значение |
| -------- | ------- |
| Общий размер хранилища  | 41_600PiB |
| Траффик в день | 38_230 Pbit/day |
| Траффик в секунду | 453 Tbit/s |
| Пиковый трафик с секунду|680 Tbit/s|
| RPS | 2_850_000 |
| Пиковый RPS | 4_250_000 |

## Глобальная балансировка

### Выбор расположения датацентров

Большая часть аудитории находится в США (28 %), другие же основные места это северная часть Южной Америки(Мексика, Бразилия) и Азия(Индия, Япония), остальной трафик равномерно приходит с Европы. 

Карта расположения ЦОД'ов Google
<img width="895" alt="image" src="https://github.com/wergit13/highload-google-drive/assets/102697969/ea7f896a-d22f-44d5-b8c7-c331e8445093">

Карта плотности населения
<img width="895" alt="image" src="https://github.com/wergit13/highload-google-drive/assets/102697969/949f0d94-6f22-4d2d-9341-8b58b2868e64">

Предлагаемое расположение ЦОД'ов для диска
<img width="895" alt="image" src="https://github.com/wergit13/highload-google-drive/assets/102697969/c07ff675-a601-4453-a5f3-af62e2941db9">

По США установим ЦОД'ы в Портланде, Техасе, Нью-Йорке и Калилифорнии как в наиболее населенных местах, в Техасе чтоб еще доставать до Мексики, а с Портланда до Канады. Еще двумя в Бельгии и в Финляндии покроем западную и северную Европы. ЦОД в Японии покроет саму Японию и восточную Россию. Сингапур же отвечает за Индию, юг Азии и Океанию. И еще один в Чили покрывает аудиторию Южной Америки.

Стоит отметить что хоть и для файлового хранилища неприятны длинные пути данных, но задержка не является критическим фактором для системы. Например ping от довольно удаленных важных городов НьюДели и Мельбурна до ближайшего ЦОД'а(Сингапура) будет 80 и 90ms соответсвенно, что является приемлемым значением.

Сайты статистики не предоставляют точных данных по странам однако если считать что насение европы прорционально США пользуются диском исходя из похожих культур получается что на один ЦОД будет приходится от 7 до 18% пользовательской аудитории. Самыми нагруженными ЦОД'адами будут 2 европейских. Тогда в самом нагруженном случае на один ЦОД будет приходится
**Примерная нагрузка на один ЦОД**

| <!-- -->      | <!-- -->        | <!-- -->      |
|:-------------:|:---------------:|:-------------:|
| Траффик в день | 6_900 Pbit/day |
| Траффик в секунду | 81.5 Tbit/s |
| Пиковый трафик с секунду|122 Tbit/s|
| RPS | 513_000 |
| Пиковый RPS | 765_000 |

### Выбор способа глобальная балансировки

Для балансировки клиентов между регионами(Америка, Европа, Азия) будем использовать Geo Based DNS (например Amazon Route 53). 

Для уменьшения задержки в пределах регионом будем использовать BGP Anycast для выбора нужного ЦОД'а. В США будет использоваться один IP для AS, который будет сопостовлять с IP настоящего ЦОД'а.
Например в случае США(самого нагруженного региона) разобьем все линки BGP сети условно на 4 группы, каждая из которых будет соответствовать одному из четырех ЦОД'ов. При загруженности (или при полном отказе одного из ЦОД'ов) мы можем скорректировать BGP веса для снижения нагрузки на ЦОД.

## Локальная балансировка

Балансировку будем делать посредством роутинга и L7 балансировщиков.

После приземления клиента в цод маршрут выбирается на основании BGP Routing'а поскольку у диска можеть быть большой входящий трафик стоит использоть сетевое оборудование. Будем использовать хэширование по IP, чтобы пакеты с одного пользователя попадали на один и тот же балансировщик сохранять кеширование SSL сессии. Роутеры будет в паре с одним вирутальным IP, резервируемым с помощью keepalived CARP.

В качестве L7 балансировщика будем использовать nginx, он же будет и отвечать за SSL терминацию. Nginx будет находится под kubernetes который будет осущесвлять liveness пробы и обновлять конфигуцию балансировщика для быстрого перекючение трафика с упавших бекэндов. Балансировщики так же будут стоять с паре через keepalived для отказоустойчивойсти.

## Логическая схемы данных
### Общая логическая схема

Схема без учета избавления от join и разбиенения на базы
<img width="1394" alt="image" src="https://github.com/wergit13/highload-google-drive/assets/102697969/e801977f-1d8a-4bf9-acd1-ae429592c0c0">


### Таблица пользователей

  * uid - id пользователя ( 16 B )
  * username - имя пользователя ( 60 B )
  * email - email пользователя ( 60 B )
  * password_hash - хеш пароля ( 32 B )
  * created_at - дата создания аккаунта ( 4 B )
  * diskroot - корневая папка диска ( 16 B)

```188 B * 2.6 bil  = 525.2 GiB ```
  
### Таблица Сессий

  * cookie - кука с которой пользователй залогинен ( 32 B )
  * expiers - дата истечения действия куки ( 4 B )
  * user_uid - пользователь который зашел с такой кукой ( 16 B )

Допустим пользователь в среднем залогинен с двух устройств

``` 52 B * 2 * 2.6 bil =  96 GiB ```

### Таблица папок

  * uid ( 16 B )
  * owner_uid  - владелец файла ( 16 B )
  * parent - родительская папка ( 16 B) 
  * name - имя папки ( 60 B )
  * created - дата создания ( 4 B )
  * share_url? ( 256 B )

Пусть у пользователя в среднем 60 папок

``` 112 B * 60 * 2.6 bil = 16 TiB ```

### Таблица файлов

  * uid ( 16 B )
  * owner_uid  - владелец файла ( 16 B )
  * parent - родительская папка ( 16 B) 
  * name - имя файла ( 60 B )
  * last_changed - дата последнего изменения ( 4 B )
  * last_version - id последней версии ( 16 B )
  * created - дата создания ( 4 B )
  * s3_url - s3 ссылка на последнюю версию ( 256 B )
  * redactor? - пользователь который послений изменил ( 16 B ) 
  * share_url? ( 256 B )

``` 190 B * 3200 ( среднее кол-во файлов у пользователя ) * 2.6 bil = 1.1 PiB ```

### Таблица версий файлов

Версий больше одной бывает только у документов которых мы считаем около 10%, пусть у документа 3 версии в среднем  

  * uid ( 16 B )
  * file_uid - файл к которому относится версия  ( 16 B )
  * user_uid - пользователь что загрузил версию ( 16 B )
  * s3_url - ссылка на s3 ( 256 B )
  * uploaded - дата изменения ( 4 B )
  * hash - хеш файла ( 32 B )

``` 328 B * 3200 * 1.2 * 2.6 bil  = 3 PiB ```

### Таблица прав доступа

Допустим только 5% файлов имеют расширенные права доступа и в среднем на 2 человек
  * share_url ( 256 B )
  * user_uid - пользователь имеющий право ( 16 B )
  * file_uid - файл или папка к которому он имеет права ( 16 B )
  * permisson - тип прав (чтение, редактировани) ( 1 B )

``` 33 B * 3200 * 0.05 * 2.6 bil = 250 TiB ```

## Физическая схема данных
### PostgreSql

В postgreSQL будем хранить талблицы files и file_versions и folders

Индексы для таблиц Folders и Files:
- id - Hash unique index - для доступа как по ключу
- parent - B-tree index для поиска всех потомков
- name - Gin index для поиска по имени

Индекс для таблицы Versions:
- id - Hash unique index
- file - B-tree index для поиска версий файла

### Tarantool

Для таблиц users, acl и sessions будем использовать Tarantool с фреймворком Cartridge

Для users ключем будет uuid

Для sessions ключем будет кука

Для acl ключем будет являтся ссылка доступа, значением будет список пар (пользователь, права) или просто общий тип доступа.

Также через Tarantool будет работать сервис координатор который будет хранить пары user:shard и определять на каком шарде postgres'а хранятся данные диска пользователя.

### S3

Сами файлы будем хранить в S3 хранилище. S3 бакеты разделяем по uuid пользователя.

### Общая схема

<img width="899" alt="image" src="https://github.com/wergit13/highload-google-drive/assets/102697969/231e2d11-89ee-499c-a736-b592dc92bf92">

Для надежности есть запасные реплики на чтение, а также запасной мастер на случай падения основного. 

С запасных реплик, поскольку они не под нагрузкой можно снимать дампы.




